# -*- coding: utf-8 -*-
"""Projet Python Final INGE 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1toTYwbi677IJ39MtQK29if6Mt-GvfQw6

## **Importation du dataset du CAC 40**
"""

# Projet : Exploration des techniques d'augmentation de donn√©es pour am√©liorer les pr√©dictions sur les march√©s financiers
# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import STL
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import seaborn as sns
from scipy.signal import periodogram
import random

"""### 1. a) Chargement des donn√©es du dataset et nettoyage de ces **donn√©es**"""

cac40_df = pd.read_csv("/CAC40_stocks_2010_2021.csv")
cac40_df['Date'] = pd.to_datetime(cac40_df['Date'])
cac40_df.set_index('Date', inplace=True)
cac40_df = cac40_df.dropna()
print("Donn√©es charg√©es et nettoy√©es.")
# V√©rifier les noms d'entreprise disponibles
print(cac40_df['CompanyName'].unique())
print(cac40_df['StockName'].unique())  # pour voir tous les noms

"""### 1.b) S√©lection de 10 titres al√©atoires"""

tickers = cac40_df['StockName'].unique()
selected_tickers = random.sample(list(tickers), 10)
print(f"Selected tickers: {selected_tickers}")
log_returns = pd.DataFrame()

"""### 1.c) Stockage des r√©sultats"""

log_returns = pd.DataFrame()
for ticker in selected_tickers:
  sub_df = cac40_df[cac40_df['StockName'] == ticker].copy()
  sub_df.sort_index(inplace=True)
  sub_df['LogReturn'] = np.log(sub_df['Close']).diff()
log_returns[ticker] = sub_df['LogReturn']
print("Log-returns calcul√©s.")

log_returns = pd.DataFrame()

for ticker in selected_tickers:
    sub_df = cac40_df[cac40_df['StockName'] == ticker].copy()
    if sub_df.empty:
        print(f"‚ùå Pas de donn√©es pour {ticker}")
        continue
    sub_df.sort_index(inplace=True)
    sub_df['LogReturn'] = np.log(sub_df['Close']).diff()

    if sub_df['LogReturn'].dropna().empty:
        print(f"‚ö†Ô∏è Pas de log return exploitable pour {ticker}")
        continue

    log_returns[ticker] = sub_df['LogReturn']
    print(f"‚úÖ Log returns ajout√©s pour : {ticker}")

"""### 2.b) Test de Dickey-Fuller (stationnarit√©) ADF"""

print("\nüìä Test de Dickey-Fuller (stationnarit√©) corrig√© :")

# On v√©rifie que le ticker est bien dans log_returns et qu‚Äôil a des valeurs exploitables
for ticker in selected_tickers:
    if ticker in log_returns.columns:
        series = log_returns[ticker].dropna()
        # Check if the series is constant before applying adfuller
        if not series.empty and not np.all(series == series.iloc[0]):
            result = adfuller(series)
            print(f"‚úÖ {ticker} - p-value : {result[1]:.4f}")
        else:
            print(f"‚ö†Ô∏è {ticker} - S√©rie vide ou constante apr√®s nettoyage.")
    else:
        print(f"‚ùå {ticker} - Donn√©es absentes dans log_returns.")

adf_test = adfuller(series)
print(f"ADF Statistic: {adf_test[0]}")
print(f"p-value: {adf_test[1]}")

"""### 3. Visualisation ACF et PACF"""

print("\n Visualisation ACF et PACF pour le premier ticker:")
example_ticker = selected_tickers[0]
series = log_returns[example_ticker].dropna()
fig, ax = plt.subplots(3, 1, figsize=(12, 10))
cac40_df[cac40_df['StockName'] == example_ticker]['Close'].plot(ax=ax[0])
ax[0].set_title(f'{example_ticker} - Prix')
plot_acf(series, ax=ax[1], lags=30)
plot_pacf(series, ax=ax[2], lags=30)
ax[1].set_title("Autocorr√©lation")
ax[2].set_title("Autocorr√©lation partielle")
plt.tight_layout()
plt.show()

"""### 3.D√©composition STL"""

print("\n D√©composition STL du log-return (composantes tendance/saisonnalit√©/bruit):")
stl = STL(series, period=30)
res = stl.fit()
res.plot()
plt.suptitle(f"{example_ticker} - STL Decomposition")
plt.tight_layout()
plt.show()

"""### 4.Analyse spectrale"""

print("\n Analyse spectrale (Periodogram):")
f, Pxx = periodogram(series)
plt.figure(figsize=(10, 4))
plt.plot(f, Pxx)
plt.title(f"{example_ticker} - Periodogram")
plt.xlabel("Fr√©quence")
plt.ylabel("Puissance")
plt.tight_layout()
plt.show()

"""### 6.Heatmap de corr√©lations crois√©es"""

print("\n Heatmap des corr√©lations entre les log-returns :")
corr_matrix = log_returns.corr()
print(corr_matrix)
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Corr√©lations des log-returns (10 titres al√©atoires)")
plt.tight_layout()
plt.show()

"""## **Partie 2 : Exploration des donn√©es financi√®res (CAC 40, S&P 500)**

### 1. Chargement et nettoyage des donn√©es
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random

# Chargement des donn√©es
cac40_df = pd.read_csv("/CAC40_stocks_2010_2021.csv")
sp500_df = pd.read_csv("/HistoricalData_1745224016092.csv")

# Nettoyage S&P 500
sp500_df.columns = ['Date', 'Close', 'Open', 'High', 'Low']
sp500_df['Date'] = pd.to_datetime(sp500_df['Date'], dayfirst=True)
sp500_df = sp500_df[::-1]  # Chronologie
if sp500_df['Close'].dtype == 'object':
    sp500_df['Close'] = sp500_df['Close'].str.replace(',', '').astype(float)
sp500_df.reset_index(drop=True, inplace=True)

# Formatage des dates CAC40
cac40_df['Date'] = pd.to_datetime(cac40_df['Date'])

# Tri
cac40_df.sort_values(by='Date', inplace=True)
sp500_df.sort_values(by='Date', inplace=True)

# Contr√¥le
print("‚úÖ CAC40 Preview:")
print(cac40_df.head())
print("‚úÖ S&P500 Preview:")
print(sp500_df.head())

"""### 2. S√©lection al√©atoire de 10 titres du CAC 40"""

# S√©lection al√©atoire de 10 titres CAC40
random.seed(42)
selected_stocks = random.sample(list(cac40_df['StockName'].unique()), 10)
cac40_sample_df = cac40_df[cac40_df['StockName'].isin(selected_stocks)].copy()

print(f"S√©lection al√©atoire : {selected_stocks}")

"""### 3. Pr√©-traitement (Log-returns & Diff√©renciation)"""

# CAC40: Log-returns et diff√©rences
cac40_sample_df['LogReturn'] = cac40_sample_df.groupby('StockName')['Close'].transform(lambda x: np.log(x / x.shift(1)))
cac40_sample_df['Diff'] = cac40_sample_df.groupby('StockName')['Close'].diff()

# S&P500: Log-returns et diff√©rences
sp500_df['LogReturn'] = np.log(sp500_df['Close'] / sp500_df['Close'].shift(1))
sp500_df['Diff'] = sp500_df['Close'].diff()

print(cac40_sample_df.head())
print(sp500_df.head())

"""### 4. Calcul des indicateurs techniques (RSI, MACD, Bollinger Bands)"""

# RSI
def RSI(series, period=14):
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)
    ma_up = up.rolling(period).mean()
    ma_down = down.rolling(period).mean()
    rsi = 100 - (100 / (1 + ma_up / ma_down))
    return rsi

# Moyennes mobiles
def SMA(series, period):
    return series.rolling(window=period).mean()

def EMA(series, period):
    return series.ewm(span=period, adjust=False).mean()

# Bollinger Bands
def BollingerBands(series, period=20):
    sma = SMA(series, period)
    std = series.rolling(window=period).std()
    upper_band = sma + (2 * std)
    lower_band = sma - (2 * std)
    return upper_band, lower_band

# MACD
def MACD(series, slow=26, fast=12, signal=9):
    ema_fast = EMA(series, fast)
    ema_slow = EMA(series, slow)
    macd_line = ema_fast - ema_slow
    signal_line = EMA(macd_line, signal)
    return macd_line, signal_line

# Appliquer sur tout le S&P 500
sp500_df['RSI'] = RSI(sp500_df['Close'])
sp500_df['SMA_20'] = SMA(sp500_df['Close'], 20)
sp500_df['EMA_20'] = EMA(sp500_df['Close'], 20)
sp500_df['BB_upper'], sp500_df['BB_lower'] = BollingerBands(sp500_df['Close'])
sp500_df['MACD'], sp500_df['MACD_signal'] = MACD(sp500_df['Close'])

# Afficher les derniers r√©sultats
print(sp500_df[['Date', 'Close', 'RSI', 'SMA_20', 'EMA_20', 'BB_upper', 'BB_lower']].tail())

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(sp500_df['Date'], sp500_df['MACD'], label='MACD')
plt.plot(sp500_df['Date'], sp500_df['MACD_signal'], label='Signal MACD')
plt.title('MACD - S&P 500')
plt.grid()
plt.legend()
plt.show()

"""###  5. Volatilit√© implicite (Rolling Std)"""

# Assuming cac40_sample_df or sp500_df contains your data
# Replace 'cac40_sample_df' with the appropriate DataFrame if needed
ticker_df = cac40_sample_df.copy() # or ticker_df = sp500_df.copy()

# Continue with your volatility calculation
ticker_df['Volatility'] = ticker_df['LogReturn'].rolling(window=20).std() * np.sqrt(252)

print(ticker_df[['Date', 'Close', 'Volatility']].tail())

"""### 6. Visualisation des s√©ries financi√®res

### *Prix de cl√¥ture CAC40*
"""

plt.figure(figsize=(12,6))
plt.plot(ticker_df['Date'], ticker_df['Close'], label=f'Prix de cl√¥ture - {ticker_example}')
plt.title(f"Prix de cl√¥ture - {ticker_example}")
plt.xlabel('Date')
plt.ylabel('Prix')
plt.legend()
plt.grid()
plt.show()

"""### *RSI & Bollinger Bands*

### 7. D√©tection et visualisation d'anomalies
"""

plt.figure(figsize=(12,6))
plt.plot(ticker_df['Date'], ticker_df['Close'], label='Prix')
plt.plot(ticker_df['Date'], ticker_df['SMA_20'], label='SMA 20', color='orange')
plt.plot(ticker_df['Date'], ticker_df['BB_upper'], label='Bollinger Band Haut', color='green')
plt.plot(ticker_df['Date'], ticker_df['BB_lower'], label='Bollinger Band Bas', color='red')
plt.fill_between(ticker_df['Date'], ticker_df['BB_lower'], ticker_df['BB_upper'], color='grey', alpha=0.1)
plt.title(f"Bollinger Bands - {ticker_example}")
plt.xlabel('Date')
plt.ylabel('Prix')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(12, 4))
plt.bar(ticker_df['Date'], ticker_df['Volume'])
plt.title(f"Volume √©chang√© - {ticker_example}")
plt.grid()
plt.show()

# Visualisation Log-returns CAC40 pour d√©tection anomalies
plt.figure(figsize=(12,6))
plt.plot(ticker_df['Date'], ticker_df['LogReturn'], color='purple', label='Log-Returns')
plt.axhline(y=0, color='black', linestyle='--')
plt.title(f"Log-returns journaliers - D√©tection d'anomalies ({ticker_example})")
plt.xlabel('Date')
plt.ylabel('Log-return')
plt.legend()
plt.grid()
plt.show()
plt.show('Log-return')

"""# PARTIE 3 : 3.2 Cr√©ation de la cible : le Close du jour suivant"""

import pandas as pd

# Chargement des donn√©es
df = pd.read_csv(r'C:\Users\manon\Documents\INGE2\ESME\Projet ML\CAC40_stocks_2010_2021.csv')

print(df.dtypes)
print(df.describe())
print(df.isnull().sum())

# Tri chronologique pour respecter la logique temporelle
df.sort_values('Date', inplace=True)#On trie les lignes du plus ancien au plus r√©cent
df.reset_index(drop=True, inplace=True) #On remet l‚Äôindex √† 0 pour avoir un DataFrame propre, o√π chaque ligne est num√©rot√©e de fa√ßon coh√©rente avec son ordre chronologique (ce qui est essentiel en s√©ries temporelles).

# Cr√©ation de la variable cible : la cl√¥ture du lendemain
df['target'] = df['Close'].shift(-1) #On cr√©e une nouvelle colonne "target" qui contient le prix de cl√¥ture du lendemain (Close d√©cal√© d‚Äôune ligne vers le haut avec shift(-1)).

# Suppression de la derni√®re ligne (target manquante)
df.dropna(inplace=True)#La toute derni√®re ligne du dataset n‚Äôa pas de target, car il n‚Äôy a pas de "lendemain" connu apr√®s le dernier jour ‚Üí elle est supprim√©e. Sinon il y aurait une ligne sans valeur √† pr√©dire ‚Üí le mod√®le ne saurait pas s‚Äôentra√Æner dessus

"""# 3.3 Construction de la baseline na√Øve

"""

# Pr√©diction na√Øve : Close de t+1 = Close de t
df['naive_pred'] = df['Close'] #Ici, j'ajoute une colonne naive_pred qui copie simplement la valeur de cl√¥ture du jour t. C‚Äôest la pr√©diction na√Øve notre t+1.

# Calcul des performances
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Fonction s√©curis√©e pour calculer le MAPE sans exploser sur des petites valeurs
def safe_mape(y_true, y_pred, epsilon=100):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    mask = y_true > epsilon  # On ignore les valeurs trop faibles
    return (np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])).mean() * 100

# Calcul des erreurs sur toute la s√©rie
naive_rmse = mean_squared_error(df['target'], df['naive_pred'], squared=False)
naive_mae = mean_absolute_error(df['target'], df['naive_pred'])
naive_mape = safe_mape(df['target'], df['naive_pred'])

# Affichage des r√©sultats
print("Baseline na√Øve :")
print(f"RMSE : {naive_rmse:.2f}")
print(f"MAE  : {naive_mae:.2f}")
print(f"MAPE : {naive_mape:.2f}%")

"""1er graphique : Code pour visualiser les pr√©dictions na√Øves vs r√©elles

"""

import matplotlib.pyplot as plt

# Taille du graphique
plt.figure(figsize=(14, 6))

# On affiche uniquement une portion des donn√©es (par exemple 100 derniers points)
n_points = 100
plt.plot(df['target'].iloc[-n_points:].values, label='Vrai Close (t+1)', marker='o')
plt.plot(df['naive_pred'].iloc[-n_points:].values, label='Pr√©diction na√Øve (Close de t)', marker='x')

plt.title("Comparaison : Close r√©el vs pr√©diction na√Øve (derniers points)")
plt.xlabel("Jour")
plt.ylabel("Prix de cl√¥ture")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""2√®me graphique : Code pour afficher les erreurs en pourcentage (MAPE journalier)

"""

# Calcul de l‚Äôerreur en % (on √©vite la division par z√©ro)
import matplotlib.pyplot as plt
df['error_percent'] = abs((df['naive_pred'] - df['target']) / df['target']) * 100
df = df[df['target'] > 1]  # Filtrage pour √©viter les % absurdes

# On prend les 100 derni√®res erreurs pour la clart√© du graphique
n_points = 100
errors = df['error_percent'].iloc[-n_points:]

# Affichage du graphique
plt.figure(figsize=(14, 5))
plt.bar(range(len(errors)), errors, color='salmon')
plt.title("Erreur en % entre la pr√©diction na√Øve et la valeur r√©elle (MAPE journalier)")
plt.xlabel("Jour (derniers points)")
plt.ylabel("Erreur (%)")
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

"""# 3.4 Pr√©paration des donn√©es d'entra√Ænement"""

#Nous s√©lectionnons maintenant les features explicatives (ici : Open, High, Low, Close, Volume) et on s√©pare nos donn√©es en train/test tout en respectant l‚Äôordre chronologique.

# S√©paration X / y
X = df[['Open', 'High', 'Low', 'Close', 'Volume']] #X : les features, c‚Äôest-√†-dire les variables que que nous utilisons pour pr√©dire. Ici : le prix d‚Äôouverture, le plus haut, le plus bas, le prix de cl√¥ture et le volume du jour t.
y = df['target'] #y : la cible, c‚Äôest-√†-dire ce qu'on veut veux pr√©dire, ici le Close du jour t+1 (target).

# D√©coupage chronologique : 80% train / 20% test
split_idx = int(0.8 * len(df)) #len(df) : donne le nombre total de lignes dans le DataFrame. 0.8 * len(df) : calcule 80‚ÄØ% de ces lignes. int(...) : transforme le r√©sultat en nombre entier (car les indices doivent √™tre des entiers).
X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

"""# 3.5 Entra√Ænement des mod√®les"""

from sklearn.linear_model import LinearRegression # un mod√®le simple et lin√©aire, qui suppose une relation proportionnelle entre les variables.
from sklearn.ensemble import RandomForestRegressor #un mod√®le d‚Äôensemble bas√© sur plusieurs arbres de d√©cision, robuste et non-lin√©aire.
from xgboost import XGBRegressor # un mod√®le tr√®s puissant de boosting, tr√®s performant sur des donn√©es tabulaires.

# Initialisation
lr = LinearRegression()
rf = RandomForestRegressor(n_estimators=100, random_state=42) #n_estimators=100 signifie qu'on utilise 100 arbres dans les deux mod√®les ensemblistes., random_state=42 permet de garder les m√™mes r√©sultats √† chaque ex√©cution (r√©p√©tabilit√©).
xgb = XGBRegressor(n_estimators=100, random_state=42)

# Entra√Ænement
#Chaque mod√®le apprend √† pr√©dire la valeur de cl√¥ture du jour suivant (target) √† partir des variables explicatives (Open, High, Low, Close, Volume).
lr.fit(X_train, y_train)
rf.fit(X_train, y_train)
xgb.fit(X_train, y_train)

# Pr√©dictions #Les trois mod√®les pr√©disent ensuite les valeurs de cl√¥ture du futur (target) sur les donn√©es de test (qu'ils n'ont jamais vues).
y_pred_lr = lr.predict(X_test)
y_pred_rf = rf.predict(X_test)
y_pred_xgb = xgb.predict(X_test)

"""# 3.6 √âvaluation des mod√®les

"""

# √âvaluation
def eval_model(name, y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    mae = mean_absolute_error(y_true, y_pred)
    mape_val = safe_mape(y_true, y_pred)
    print(f"{name} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, MAPE: {mape_val:.2f}%")

eval_model("R√©gression Lin√©aire", y_test, y_pred_lr)
eval_model("Random Forest", y_test, y_pred_rf)
eval_model("XGBoost", y_test, y_pred_xgb)

"""Comparaison des pr√©dictions des diff√©rents mod√®les avec la r√©alit√©

"""

import matplotlib.pyplot as plt
plt.figure(figsize=(14, 6))

# Derniers points pour la clart√© du graphique
n_points = 100

# Affichage des vraies valeurs de la cible (Close r√©el du jour suivant)
plt.plot(y_test.iloc[-n_points:].values, label='Vrai Close (t+1)', marker='o', color='blue')

# Affichage des pr√©dictions des trois mod√®les
plt.plot(y_pred_lr[-n_points:], label='Pr√©diction R√©gression Lin√©aire', marker='x', color='green')
plt.plot(y_pred_rf[-n_points:], label='Pr√©diction Random Forest', marker='x', color='red')
plt.plot(y_pred_xgb[-n_points:], label='Pr√©diction XGBoost', marker='x', color='orange')

# graphique
plt.title("Comparaison des pr√©dictions des mod√®les vs R√©el (derniers points)", fontsize=14)
plt.xlabel("Jour (derniers points)", fontsize=12)
plt.ylabel("Prix de Cl√¥ture", fontsize=12)
plt.legend(loc='best')
plt.grid(True)
plt.tight_layout()

plt.show()

"""#3.7 Mod√®le ARIMA (classique des s√©ries temporelles)

"""

from statsmodels.tsa.arima.model import ARIMA


# Utilisation de la s√©rie Close uniquement
serie = df['Close'] #On extrait uniquement la colonne des prix de cl√¥ture du DataFrame, car ARIMA travaille sur des s√©ries temporelles univari√©es (une seule variable √† la fois).

# On coupe la s√©rie en train/test
train_arima = serie[:split_idx] #On d√©coupe les donn√©es en deux : train_arima : partie utilis√©e pour entra√Æner le mod√®le (jusqu‚Äô√† split_idx) , test_arima : partie pour tester/pr√©dire, donc le mod√®le ne l‚Äôa jamais vue
test_arima = serie[split_idx:] #On ne peut pas utiliser la m√©thode train_test_split car s√©rie temporelle donc d√©coupage chrolologique

# ARIMA simple (p=1, d=1, q=0 par exemple)
model_arima = ARIMA(train_arima, order=(1,1,0)) #p=1 : nombre de termes autoregressifs (d√©pendance √† la valeur pr√©c√©dente); d=1 : nombre de diff√©renciations pour rendre la s√©rie stationnaire; q=0 : nombre de termes de moyenne mobile (pas utilis√© ici). #Ce mod√®le suppose que la s√©rie d√©pend surtout de ses valeurs pass√©es (autoregression), avec une diff√©renciation pour √©liminer les tendances.
model_fit = model_arima.fit() #On entra√Æne le mod√®le ARIMA sur les donn√©es d'entra√Ænement.

# Pr√©diction
forecast_arima = model_fit.forecast(steps=len(test_arima)) #Le mod√®le pr√©voit la suite de la s√©rie sur len(test_arima) jours, c‚Äôest-√†-dire autant de points que la partie test.

# √âvaluation
eval_model("ARIMA", test_arima, forecast_arima) #On √©value les performances de la pr√©diction ARIMA avec la fonction eval_model() (qui utilise probablement safe_mape()).

"""graphique qui compare les pr√©dictions ARIMA √† la valeur r√©elle des prix de cl√¥ture dans la p√©riode de test

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(test_arima.index, test_arima, label='R√©alit√©', color='black')
plt.plot(test_arima.index, forecast_arima, label='Pr√©diction ARIMA', color='green', linestyle='--')

plt.title("Comparaison des pr√©dictions ARIMA avec la r√©alit√©")
plt.xlabel("Date")
plt.ylabel("Prix de cl√¥ture")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()